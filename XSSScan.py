#!/usr/bin/env python3
#

from bs4 import BeautifulSoup
from urllib.parse import urlparse
from urllib.parse import urljoin
import requests, sys, os, atexit, optparse
from http.cookies import SimpleCookie
import re
import argparse
import core.Config
from core.Log import setup_logger
from core.Colors import end, red, white, bad, info
logger = setup_logger()



# Import everything else required from core lib
from core.Config import payloads
from core.Prompt import Prompt
from core.Utils import extractHeaders, converter
#globalVariables = vars(args)
from modes.Scanning import scan
payloadList = payloads
requests.packages.urllib3.disable_warnings()


def readlinks (url,headers):
  try:
    r = requests.get(url, headers=headers, verify=False)
    data = r.text
    soup = BeautifulSoup(data, "lxml")
    parsed_uri = urlparse(url)
    domain = '{uri.netloc}'.format(uri=parsed_uri)
    domain = domain.split(':')[0]
  except Exception as ex:
    print(ex)
  for form in soup.find_all('form'):
    datas=""
    for tag in form.find_all():
      data=tag.get('name')
      if data is not None:
        datas+=data+"=&"
    form_data={'method':'GET','url':url,'data':datas[0:-1]}
    if (form.get('method')): 
      form_data['method']=form.get('method')
    if "http" in form.get('action'):
      if domain in urlparse(form.get('action')):
        form_data['url']=form.get('action')
      else: continue
    else:
      form_data['url']=urljoin(url,form.get('action'))
    if (form_data['url'] not in url_save): 
      url_save.append(form_data['url'])
    if (form_data not in list_req):
      list_req.append(form_data)
  # PARSE LINKS
  for link in soup.find_all('a'):
    form_data={'method':'GET','url':url,'data':''}
    # IF LINK IS NOT NULL
    if link.get('href') is not None:
      parsed_uri = urlparse(link.get('href'))
      # IF LINK STARTS WITH HTTP
      if link.get('href')[:4] == "http":
        # SAME ORIGIN
        if domain in link.get('href'):
          # IF URL IS DYNAMIC
          if "?" in link.get('href'):
            form_data['url']=link.get('href').split('?')[0]
            form_data['data']=form_data['data']="=&".join(re.split("=\S*&",link.get('href').split('?')[1]+"&"))[:-1]
            if (form_data not in list_req): 
              list_req.append(form_data)
            if (form_data['url'] not in url_save): 
              url_save.append(form_data['url'])
          else:
            form_data['url']=link.get('href')
            if (form_data['url'] not in url_save): 
              url_save.append(form_data['url'])
            if (path and form_data not in list_req): 
              list_req.append(form_data)
      # IF URL IS DYNAMIC
      elif "?" in link.get('href'):
        form_data['url']=urljoin(url,link.get('href').split('?')[0])
        form_data['data']="=&".join(re.split("=\S*&",link.get('href').split('?')[1]+"&"))[:-1]
        if (form_data['url'] not in url_save): 
          url_save.append(form_data['url'])
        if (form_data not in list_req): 
          list_req.append(form_data)
      elif link.get('href')[:1] == "/":
        form_data['url']=urljoin(url,link.get('href'))
        if path and form_data not in list_req: 
          list_req.append(form_data)
        if (form_data['url'] not in url_save): 
          url_save.append(form_data['url'])

globalURL = "globalBadness"
if len(sys.argv) < 2:
  print("You need to specify a URL to scan. Use --help for all options.")
  quit()
else:
  parser = argparse.ArgumentParser()
  parser.add_argument('-u', '--url', help='Full URL', dest='url')
  parser.add_argument('-d', '--domain', help='Domain name', dest='domain')
  parser.add_argument('--timeout', help='timeout',
                      dest='timeout', type=int, default=core.Config.timeout)
  parser.add_argument('--proxy', help='use prox(y|ies)',
                      dest='proxy', action='store_true')
  parser.add_argument('--port', help='use port',
                      dest='port', type=int, default=80)
  parser.add_argument('--headers', help='add headers',
                      dest='add_headers', nargs='?', const=True)
  parser.add_argument('--delay', help='delay between requests',
                      dest='delay', type=int, default=core.Config.delay)
  parser.add_argument('--path', help='inject payloads in the path',
                    dest='path', action='store_true')
  args = parser.parse_args()

  options = parser.parse_args()
  target = str(options.url)
  domain = str(options.domain)
  #scan = str(options.scan)
  path = args.path
  port = str(options.port)
  delay = options.delay
  proxy = options.proxy
  timeout = options.timeout
  if not proxy:
    core.Config.proxies = {}
  #ans = scan
  level = 1
  if type(args.add_headers) == bool:
    headers = extractHeaders(Prompt())
  elif type(args.add_headers) == str:
    headers = extractHeaders(args.add_headers)
  else:
    from core.Config import headers
  #using a domain and a port or a URL?
  if ":" not in target:

    if (len(str(domain)) > 4):
      target = "http://" + domain
      #print "target is: " + target
    else:
      parsed_uri = urlparse(target)
      domain = '{uri.netloc}'.format(uri=parsed_uri)
      #print "domain after parsed_uri is now: " + domain

    if (len(str(target)) > 6):
      url = target + ":" + port #big change here
      #print("url is: " + url)
    else:
      url = "http://" + str(domain) + ":" + port
      #print "url is: " + url
  else:
      url = target
      globalURL = target
      #print "url is: " + url
      parsed_uri = urlparse(target)
      domainWithPort = '{uri.netloc}'.format(uri=parsed_uri)
      domain = domainWithPort.split(':')[0]
      #print "domain after parsed_uri is now: " + domain
      if (len(target.split(':')) > 2):
        portWithPossiblePath = target.split(':')[2]
        port = portWithPossiblePath.split('/')[0]
        #print "port is: " + port
      else:
        port = port
        #print "port is: " + port

  
  # FILE INIT
  url_save=[]
  list_req=[]
  scaned=[]
  url_save.append(url)
  i=0
  j=0
  while (i<len(url_save)):
    try:
      readlinks(url_save[i],headers)
      while (j<len(list_req)):
        if (list_req[j]['data']==''):
          paramData = converter(list_req[j]['url'], list_req[j]['url'])
          core.Config.globalVariables['path']=True
        elif list_req[j]['method']=="GET":
          paramData=""
          list_req[j]['url']+="/?"+list_req[j]['data']
          core.Config.globalVariables['path']=False
        else: 
          paramData=list_req[j]['data']
          core.Config.globalVariables['path']=False
        saved=list_req[j]['url']+paramData if not core.Config.globalVariables['path'] else list_req[j]['url']
        if (saved) not in scaned:
          logger.info("Target: "+list_req[j]['url'])
          scan(list_req[j]['url'], paramData, 0, headers, delay, timeout, 1, 0, 0)
          scaned.append(saved)
        j+=1
    except IndexError:
      pass
    i+=1